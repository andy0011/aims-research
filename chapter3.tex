\chapter{Face Recognition}
In this chapter, three approaches to the face recognition process are described, namely; Eigenfaces method for face recognition, Fisherfaces method and Local binary patterns (LBP).

The Eigenfaces method for face recognition is based on Principal Component Analysis (PCA), which linearly projects a high-dimensional image space into a low-dimensional feature space called face space aiding efficient feature classification. The use of eigenfaces for face recognition was pioneered by Kirby and Sirovich \citep{sirovich19} and used by Turk and Pentland \citep{turk19} in automated face recognition systems. 

The Eigenfaces method is known by many as the first successful facial recognition technology which has served as the basis for one of the top commercial face recognition technology products. There have been many extensions and many new developments in automatic recognition systems using the Eigenfaces method since its initial development. The name Eigenfaces is given to the set of eigenvectors obtained from the covariance matrix of the probability distribution over the high-dimensional vector space of face images.

The Fisherfaces method, proposed by Belhumeur, Hespanha and Kriegman, \citep{belhu} is based on Fisher's linear discriminant (FLD) for face recognition. The Fisherfaces method uses discriminant analysis to project the initial high-dimensional image space into the directions that best separate the classes, by maximizing the ratio of between-class scatter to that of within-class scatter. 

Local Binary Pattern (LBP) is a texture description technique proposed originally for texture analysis. It was introduced in 1996 and 2002 \citep{ojala2} based on the assumption that texture has locally two complementary aspects, a pattern and its strength. In image analysis, LBP is used to determine the local features in facial images. The facial image  is divided into local regions from which LBP histograms (LBP features) are formed and concatenated into a single feature vector (histogram) which efficiently represent the face image. The feature vector is then used to measure the similarities by calculating the distance between the images.




\section{Eigenfaces}
Eigenface recognition technique is based on the use of principal component analysis (PCA). Principal component analysis, (PCA), is a linear transformation technique often used for feature extraction and data dimension reduction problems. PCA was proposed by Karl Pearson (1901) and Harold Hotelling (1933).  Its goal is to extract important features from the image and to express this features as a set of new orthogonal variables called principal components \citep{turk19}. Thus, given  a set $\mathbf{X}$ consisting of $M$ images, PCA produces a new set of images known as feature space, and the elements in these space are uncorrelated and are ordered in terms of the amount of variance they explain from the original image set. The important features extracted are known as "Eigenfaces" because they are the eigenvectors of the image set.

%PCA is performed on the training set, after extracting the feature
%transformation and all the eigenvectors with nonzero eigenvalues %are used as the candidates for constructing random PCA subspaces. 
%The eigenface technique, another method based on linearly projecting the image space to a low dimensional subspace, has similar computational requirements

%The system functions by projecting face images onto a feature space that spans the significant variations among known face images. The significant features are known as "eigenfaces," because they are the eigenvectors (principal components) of the set of faces; they do not necessarily correspond to features such as eyes, ears, and noses. The projection operation characterizes an individual face by a weighted sum of the eigenface features, and so to recognize a particular face it is necessary only to compare these weights to those of known individuals. Some particular advantages of our approach are that it provides for the ability to learn and later recognize new faces in an unsupervised manner, and that it is easy to implement using a neural network architecture.


\subsection{Calculating Eigenfaces (Principal components)}
Let $\mathbf{X}=\lbrace \mathbf{x}_1, \mathbf{x}_2, \cdots, \mathbf{x}_M\rbrace$ be a set of $M$ sample face images formed by the vectors $\mathbf{x_k}\in \mathbb{R}^N$. The average $\mathbf{\bar{x}}$ (mean face) and the covariance matrix $\mathrm{Cov}(\mathbf{x})= \mathbf{\Sigma}$ of these image set are defined by: \begin{align*}
\mathbf{\bar{x}} &= \frac{1}{M}\sum_{n=1}^{M} \mathbf{x_n} 
\end{align*} and \begin{align*}
\mathbf{\Sigma}& = \frac{1}{M}\sum_{n=1}^{M}(\mathbf{x_n} -\mathbf{\bar{x}})(\mathbf{x_n} -\mathbf{\bar{x}})^T
\end{align*}
respectively. Then, each face image differs from the mean face by the vector $\mathbf{\phi}_i = \mathbf{x}_i -\mathbf{\bar{x}}$, where $i = 1, 2, \ldots, M$. PCA seeks a linear transformation that maps the original $N-$dimensional image space into a $d-$dimensional feature space, where $d<N$, with minimum loss of information. Thus, PCA seek vectors, $\mathbf{w}$, such that the sample, after projection onto $\mathbf{w}$, is most spread out. 
The projection of all the data points onto this dimension can be written as: \begin{align*}
\mathbf{z} = \mathbf{w}^T \mathbf{x}_n
\end{align*}
and the variance of the projected data is given by: \begin{align}
\mathrm{var}(\mathbf{z}) = \mathbf{w}^T \mathbf{\Sigma}\mathbf{w}.
\end{align}
We seek $\mathbf{w}$ that maximized $\mathrm{var}(\mathbf{z})$   subject to the constraint that $\mathbf{w}^T\mathbf{w}=1$ (since $|| \mathrm{w}|| \longrightarrow \infty$). Writing this as optimization problem, we have \begin{align}
\mathrm{max}\; &\mathbf{w}^T \mathbf{\Sigma}\mathbf{w} \notag\\
\mathrm{subject \; to}\; &\mathbf{w}^T\mathbf{w}= 1.
\end{align}

Introducing the Lagrangian multiplier, denoted by $\lambda$, and writing $(3.1.2)$ as Lagrange problem, we have \begin{align*}
L(\mathbf{w}, \lambda)= \mathbf{w}^T \mathbf{\Sigma}\mathbf{w}-\lambda (\mathbf{w}^T\mathbf{w}-1).
\end{align*}
Taking the derivative with respect to $\mathbf{w}$ and setting it equal to $0$, we have \begin{align*}
2\mathbf{\Sigma}\mathbf{w} - 2\lambda \mathbf{w} &=0\\
2\mathbf{\Sigma}\mathbf{w} &= 2\lambda \mathbf{w}\\
\Longrightarrow \mathbf{\Sigma}\mathbf{w} &= \lambda \mathbf{w}
\end{align*}
where $\mathbf{w}$ is an eigenvector of $\mathbf{\Sigma}$ and $\lambda$ is the corresponding eigenvalue.
Pre-multiplying both sides by $\mathbf{w}^T$ we have \begin{align*}
\mathbf{w}^T \mathbf{\Sigma}\mathbf{w} = \lambda \mathbf{w}^T\mathbf{w} = \lambda
\end{align*}
and so the variance will be a maximum when we set $\mathbf{w}$ equal to the eigenvector with the highest eigenvalue $\lambda$. Therefore the first principal component is given by the eigenvector with the largest associated eigenvalue of the sample covariance matrix $\mathbf{\Sigma}$. A similar argument can show that the $k$ dominant eigenvectors of covariance matrix $\mathbf{\Sigma}$ determine the first $k$ principal components. In general \begin{align*}
\mathrm{z_k} = \mathbf{W}^T\mathbf{\Sigma}\mathbf{W} = \lambda_k
\end{align*}
where $\mathbf{W}=[\mathbf{w}_1, \mathbf{w}_2, \cdots , \mathbf{w}_k]$.

\subsection{Recognition}
The Eigenfaces forms a basis for the new dimensional feature space (subspace) and spans the original data set. The images in the original data are linear combinations of the feature vectors (Eigenfaces / weights vector). A new input data forms a linear combination of these weight vectors and compare distances between the images in the training set. The closer the images, the more likely it recognizes as a facial image.

The projection operation characterizes an individual face by a weighted sum of the eigenface features, and so to recognize a particular face it is necessary only to compare these weights to those of known individuals.

%\begin{thm}[My Theorem2]
%This is my theorem2.
%\end{thm}
%\begin{proof}
%And it has no proof2.
%\end{proof}



%\begin{align} % do not use eqnarray. 
%\label{2ya}
%x & = y + y\\
%\label{2yb}
%& = 2y
%\end{align}
%see equations \eqref{2ya} and \ref{2yb}

\section{Fisherfaces}
Fisherfaces technique technique is a feature extraction technique based on Fisher's linear discriminant analysis (FLDA). It seeks to find a projection, $\mathbf{w}$, that maximizes the between-class separability and minimizes the within-class variability. The technique first project the face images from the original vector space to a lower dimensional space (face subspace) using Principal Component Analysis (PCA) and then LDA is applied next to find the best linear discriminant features on that face subspace. Recognition is then done by using nearest neighbor (NN) classifier in this final subspace.

%Fisher linear discriminating (FLD, Fisherface) approach maps the feature to subspaces that most separate the two classes.

\subsection{Calculating Fisherfaces}
Consider two classes with data points $n_1$ and $n_2$. Let $\lbrace \mathbf{x}_1, \mathbf{x}_2, \cdots, \mathbf{x}_M\rbrace$, $\mathbf{x}\in \mathbb{R}^N$ be the set of $M$ face images belonging to the two classes, $C_1$ and $C_2$.
FLD seeks a direction, defined by the vector $\mathbf{w}$, such that when the data are projected onto $\mathbf{w}$, the data points from the two classes are as well separated as possible.
We define \begin{align*}
\mathbf{z} = \mathbf{w}^T\mathbf{x} = \sum_{k=1}^{M}\mathbf{w}_k\mathbf{x}_k
\end{align*}
to be the projection of $\mathbf{x}$ onto $\mathbf{w}$ and thus a dimensionality reduction from $N$ to $1$. 
The means for the two classes are defined as: \begin{align*}
\mu_1 = \frac{1}{n_1}\sum_{k\in C_1}\mathbf{x}_k \quad \mathrm{and}\quad \mu_2 = \frac{1}{n_2}\sum_{k\in C_2}\mathbf{x}_k
\end{align*}
and the means after projection are given by \begin{align*}
\mathbf{w}^T\mu_1 \quad \mathrm{and} \quad \mathbf{w}^T\mu_2
\end{align*}
respectively. The covariance matrix after projection are: \begin{align*}
\mathbf{\Sigma}_1 = \mathbf{w}^T\mathbf{\Sigma}_1\mathbf{w}
\end{align*}
and \begin{align*}
\mathbf{\Sigma}_2 = \mathbf{w}^T\mathbf{\Sigma}_2\mathbf{w}.
\end{align*}
For the two classes to be as well separated as possible after projection, we would like the means to be as far as possible and the data points of the classes be scattered in as small a region as possible.
Thus, the distance between the projected class means is: \begin{align*}
\left(\mathbf{w}^T\mu_1- \mathbf{w}^T\mu_2\right)^2 &=\left(\mathbf{w}^T\mu_1-\mathbf{w}^T\mu_2\right)^T\left(\mathbf{w}^T\mu_1-\mathbf{w}^T\mu_2\right)\\
&= \left(\mu_1-\mu_2\right)\mathbf{w}\mathbf{w}^T\left(\mu_1-\mu_2\right)\\
&= \mathbf{w}^T\left(\mu_1-\mu_2\right)\left(\mu_1-\mu_2\right)\mathbf{w}\\
& = \mathbf{w}^T S_b\mathbf{w}
\end{align*}
where $S_b=\left(\mu_1-\mu_2\right)\left(\mu_1-\mu_2\right)$ is the between class variance. Since we want to minimize the variance within each class, we write \begin{align*}
\mathbf{w}^T\mathbf{\Sigma}_1\mathbf{w} +\mathbf{w}^T\mathbf{\Sigma}_2\mathbf{w} &= \mathbf{w}^T\left(\mathbf{\Sigma}_1+\mathbf{\Sigma}_2\right) \mathbf{w}\\
&= \mathbf{w}^TS_w \mathbf{w}
\end{align*}
where $S_w=\mathbf{\Sigma}_1+\mathbf{\Sigma}_2$ is the within class covariance. Fisher's Linear discriminant is $\mathbf{w}$ such that the distance between projected class means are maximize and the within class variance are minimize by maximizing the ratio:
\begin{align*}
\max_{\mathbf{w}}\; \frac{\mathbf{w}^TS_b\mathbf{w}}{\mathbf{w}^TS_w\mathbf{w}}
\end{align*}
and this is equivalent to finding :\begin{align*}
\max_{\mathbf{w}}\; &\mathbf{w}^TS_b\mathbf{w}\\
\mathrm{s.t.}\; & \mathbf{w}^TS_w\mathbf{w}=1.
\end{align*}
Applying Lagrange multiplier, denoted by $\lambda$, we have \begin{align*}
L(\mathbf{w}, \lambda) = \mathbf{w}^TS_b\mathbf{w} -\lambda\left(\mathbf{w}^TS_w\mathbf{w}-1\right). 
\end{align*}
Taking the derivative with respect to $\mathbf{w}$ and setting to zero, we get: \begin{align*}
2S_b\mathbf{w} - 2\lambda S_w\mathbf{w}=0\\
S_b\mathbf{w}= \lambda S_w\mathbf{w}
\end{align*}
which is the generalized eigenvalue problem that is equivalent to (for $S_w$ non-singular):\begin{align*}
S_w^{-1}S_b\mathbf{w} =\lambda \mathbf{w}
\end{align*}
where $\lambda$ and $\mathbf{w}$ are eigenvalues and eigenvectors of $S_w^{-1}S_b$ respectively. $\mathbf{w}$ is the eigenvector corresponding to the largest eigenvalue of $S_w^{-1}S_b$.

\subsection{Recognition}

\section{Local Binary Patterns (LBP)}
In the local binary patterns (LPB) technique for face description, the face image is divided into small blocks called local regions, from which LBP descriptors are extracted into histograms. The obtained histograms from the local regions are concatenated into a single feature histogram, which gives a global description of the face image (Figure 3.1). During image recognition, images are compared by measuring the similarity (distance) between their histograms.
%This description of the face is used to measure similarities between images. Recognition is then done by computing simple histogram similarities.

The method considers a $3\times 3$ block of pixels and then LBP is derived by comparing the center pixel with its neighbors to derive a code which is stored at the center pixel. A neighboring pixel is assigned the binary number $1$ if its intensity is greater than or equal to the center pixel and $0$ otherwise. This gives a binary pattern of eight digit with $2^8 = 256$ possible combinations called Local Binary Patterns or LBP codes.
%Based on the intensity of the neighboring pixels, ones are assigned to the neighbor pixel if the central pixel is less and 1 otherwise. 
%This histogram effectively has a description of the face on three different levels of locality: the LBP labels for the histogram contain information about the patterns on a pixel-level, the labels are summed over a small region to produce information on a regional level and the regional histograms are concatenated to build a global description of the face.

%It should be noted that when using the histogram based methods the regions do not need to be rectangular. Neither do they need to be of the same size or shape, and they do not necessarily have to cover the whole image. It is also possible to have partially overlapping regions.

%In this work, we present a novel approach to face recognition which considers both shape and texture information to represent face images. The face area is first divided into small regions from which Local Binary Pattern (LBP) histograms are extracted and concatenated into a single, spatially enhanced feature histogram efficiently representing the face image. The recognition is performed using a nearest neighbour classifier in the computed feature space with Chi square as a dissimilarity measure. 
\begin{figure}[!h]
\centering 
\includegraphics[width=0.6\textwidth]{images/h3.png}
\caption{Face image divided into 9 regions with every region representing a histogram and then a concatenated histogram.}
\label{lbp1} 
\end{figure}

\subsection{Calculating LBP}
For the central pixel $P$ and the neighboring pixels $\mathbf{P}_x$, the process depends on thresholding, which is the function 
\begin{align}
\rho(x) =
\begin{cases}
1, & \mathbf{P}_x \geq P
\\
0, & \mathrm{Otherwise}.
\end{cases}
\end{align}
The LBP code is derived as a result of binary weighting applied to the result of thresholding which is equivalent to thresholding the points neighboring the center point and then unwrapping the code as a binary code. The LBP code for a point $P$ with
eight neighbors $x$ is then given by 
\begin{align} 
\mathrm{LBP} & = \sum_{x\in {1,8}}\rho (x)\times 2^{x-1}.\\
\end{align} 

\begin{figure}[!h]
\centering 
\includegraphics[width = 0.8\textwidth]{images/lbp1.png}
\caption{The original LBP operator.}
\label{lbp} 
\end{figure}
From Fig. \ref{lbp}, the binary code and the LBP code are calculated from the weights ($2^p$) as follows:
\begin{align*}
\mathrm{Binary \; code}&: 10110010\\
\mathrm{LBP \; code} &: 1\times 2^7+ 1\times 2^6 + 0\times 2^5+ 0\times 2^4 + 0\times 2^3 + 0\times 2^2 + 1\times 2^1+ 1\times 2^0\\
& = 128 + 64 + 2 + 1\\
& = 195.
\end{align*}

\subsection{Recognition}
Face images are identified using $k$-nearest neighbor classifier by majority vote of its neighbors. Consider a training set $X =\lbrace \mathbf{I}_1, \mathbf{I}_2, \ldots, \mathbf{I}_m\rbrace$ of $m$ different people with each $\mathbf{I}$ having $n$ images. Given an input image $Y$ that is to be identified, the distances among all the $n = n_1 +n_2+ \cdots + n_m$ images and $Y$ are computed. Then, the person of image $Y$ is identified as the person with the most images in the $k$-nearest neighbors. The distance between the histograms are computed by the use of Euclidean distance. The histogram distance between the histogram $H_i$ of image $Y$ which is to be identified and the histogram $H_o$ of the image $O$ which is given in the  training set is calculated as: \begin{align*}
D_E (Y, O) = \sqrt{\sum_{k=2}^{l}(H_i(k)-H_d(k))^2}.
\end{align*}













